{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ziatdinovmax/gpax/blob/main/examples/GPax_MultiTaskGP_BO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For github continuous integration only\n",
    "# Please ignore if you're running this notebook!\n",
    "import os\n",
    "if os.environ.get(\"CI_SMOKE\"):\n",
    "    NUM_WARMUP = 100\n",
    "    NUM_SAMPLES = 100\n",
    "    SMOKE = True\n",
    "else:\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = 2000\n",
    "    SMOKE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX88m4Rfnog7"
   },
   "source": [
    "# \"Theory-informed\" data reconstruction and Bayesian optimization with multi-task GP\n",
    "\n",
    "*Prepared by Maxim Ziatdinov (July 2023). Last updated in October 2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The purpose of this notebook is to demonstrate a concept of theory-guided Bayesian optimization when theoretical simulations are available beforehand and are used to guide the measurements through a multi-task Gaussian process. This can be used as an alternative solution to a structured Gaussian process in situations where a mean function is too costly to compute at each step or it is expressed through some complex program that is not fully differentiable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOXxpd-XNKel"
   },
   "source": [
    "In the field of Gaussian Processes (GPs), the Linear Model of Coregionalization (LMC) provides an effective way to capture correlations across multiple outputs. This model can be particularly useful in situations where you have an abundance of \"cheap\" simulations performed over a dense grid in a parameter space and a small number of \"expensive\" experimental measurements in that space. In such scenarios, the goal is to leverage the simulations to assist in data reconstruction or to guide active learning in automated experiments.\n",
    "\n",
    "In the LMC-based GP, the covariance between the $i$-th output of the $n$-th data point, $x_n$, and the $j$-th output of the $m$-th data point, $x_m$, can be formulated as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "K[Y_i(x_n), Y_j(x_m)] = \\sum_{q=1}^{Q} B_{ij}^{(q)} k_q(x_n, x_m)\n",
    "\\end{align*}\n",
    "\n",
    "In the above equation, $k_q(x_n, x_m)$ denotes the covariance function for the $q$-th latent process, evaluated between the $n$-th and $m$-th data points. The term $B_{ij}^{(q)}$ corresponds to the element at the $i$-th row and $j$-th column of the coregionalization matrix, $B^{(q)}$, for the $q$-th latent process. This matrix establishes the correlation structure among the various outputs and encodes their interdependencies. Each component of $B^{(q)}$ quantifies the correlation between two tasks as influenced by the $q$-th latent process.\n",
    "\n",
    "To ensure the symmetric and positive semi-definiteness of $B^{(q)}$, it is parameterized as\n",
    "\n",
    "\\begin{align*}\n",
    "B^{(q)} = W^{(q)} (W^{(q)})^T + \\text{diag}(v^{(q)})\n",
    "\\end{align*}\n",
    "\n",
    "In this formulation, $W^{(q)}$ is a low-rank $D \\times R$ matrix, with $D$ representing the number of tasks, and $R$ being the rank. $\\text{diag}(v^{(q)})$ is a diagonal matrix encapsulating specific variances for each output. Both the $W$ and $v$ matrices, along with the traditional data kernel hyperparameters and task-specific noise levels, are learned directly from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdtH0tCPQ2de"
   },
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86iUwKxLO7qE"
   },
   "source": [
    "Install the latest GPax package from PyPI (this is best practice, as it installs the latest, deployed and tested version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ1rLUzqha2i",
    "outputId": "44157aab-4e21-4966-ec79-ccf85cd4bbaa"
   },
   "outputs": [],
   "source": [
    "!pip install gpax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vygoK7MTjJWB"
   },
   "source": [
    "Import needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For use on Google Colab\n",
    "    import gpax\n",
    "\n",
    "except ImportError:\n",
    "    # For use locally (where you're using the local version of gpax)\n",
    "    print(\"Assuming notebook is being run locally, attempting to import local gpax module\")\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    import gpax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de6SYYO6Ojts"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpax.utils.enable_x64()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable some pretty plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOqGfOVDPpPA"
   },
   "source": [
    "## Theory-informed data reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAycs5AZ9xuv"
   },
   "source": [
    "Let's create toy datasets. The idea is that we have a cheap theoretical simulation that we can perform over the entire parameter space and costly experiments that we can measure only in some parts of that parameter space. The theory and experiment do not match 100%. We want to use multi-fidelity/task GP to learn the correlation between the theoretical simulations and available experimental observations and use this information to aid predictions in the unmeasured parts of the parameter space.\n",
    "\n",
    "*Please note that our data is synthetic and our \"simulations\" and \"experiment\" are not actual simulations and actual measurements. However, you can susbtitute them with your actual data and run the same code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "OJN7ZvqdOiB6",
    "outputId": "55f69c65-5a6f-459d-af6d-4962f6e2313f"
   },
   "outputs": [],
   "source": [
    "func = lambda x: 2 * np.sin(x/10) + 0.5 * np.sin(x/2) + 0.1 * x\n",
    "\n",
    "def y1_func(x, noise=0):\n",
    "    # Assume signal strength is some complex function of frequency x\n",
    "    return func(x) + np.random.normal(0, noise, x.shape)\n",
    "\n",
    "def y2_func(x, noise=0):\n",
    "    return 1.5 * func(x) + np.sin(x/15) - 5 + np.random.normal(0, noise, x.shape)\n",
    "\n",
    "np.random.seed(1) # for reproducibility\n",
    "\n",
    "# Fidelity 1 - \"theoretical model\"\n",
    "X1 = np.linspace(0, 100, 100)\n",
    "y1 = y1_func(X1)\n",
    "\n",
    "# Fidelity 2 - \"experimental measurements\"\n",
    "X2 = np.concatenate([np.linspace(0, 25, 20), np.linspace(75, 100, 20)])  # only have data for some frequencies\n",
    "y2 = y2_func(X2, noise=0.3)\n",
    "\n",
    "# Ground truth for Fidelity 2\n",
    "X_full_range = np.linspace(0, 100, 200)\n",
    "y2_true = y2_func(X_full_range)\n",
    "\n",
    "\n",
    "# Add fidelity indices\n",
    "X = np.vstack(\n",
    "    (np.column_stack((X1, np.zeros_like(X1))),  # add indices associated with the fidelity\n",
    "     np.column_stack((X2, np.ones_like(X2))))   # add indices associated with the fidelity\n",
    ")\n",
    "\n",
    "# We will pass target values to GP as a single array\n",
    "y = np.concatenate([y1, y2]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X1, y1, 'b-', label='Theoretical Model (Fidelity 1)', alpha=0.6)\n",
    "plt.scatter(X2, y2, c='k', label='Experimental Data (Fidelity 2)', alpha=0.6)\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True function (Fidelity 2)', linewidth=2)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Signal Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2E19VSodQXJ"
   },
   "source": [
    "First, let's run a regular GP for the experimental data alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfywkGt-dTbb",
    "outputId": "ba9548b1-9460-4791-8f26-ba4f913a0f6f"
   },
   "outputs": [],
   "source": [
    "key1, key2 = gpax.utils.get_keys(1)\n",
    "\n",
    "model = gpax.ExactGP(1, kernel='Matern')\n",
    "model.fit(key1, X2, y2, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjEc4FV--khW"
   },
   "source": [
    "Make a prediciton with the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-1E35lgdYFn"
   },
   "outputs": [],
   "source": [
    "# Make a prediction with the trained model\n",
    "y_mean, y_sampled = model.predict(key2, X_full_range, noiseless=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kLprHeX-m1_"
   },
   "source": [
    "Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "G0kalzH-fwVW",
    "outputId": "95919dea-f52a-4e5a-a3e0-02d23eeb41dc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.plot(X_full_range, y_mean, c='red', label=\"Vanilla GP prediction\")\n",
    "plt.fill_between(X_full_range,\n",
    "                y_mean - y_sampled.squeeze().std(0),\n",
    "                y_mean + y_sampled.squeeze().std(0),\n",
    "                alpha=0.3, color='red')\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='Ground Truth')\n",
    "plt.scatter(X2, y2, c='k', label='Experimental Data (Fidelity 2)', alpha=0.6)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Signal Strength')\n",
    "plt.title(\"Vanilla GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r923zNNlSdPn"
   },
   "source": [
    "Now let's train a multi-task/fidelity GP model. Start with a single latent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9Rdc2FhcB4_",
    "outputId": "1c2100b3-0ea1-4324-d6ba-aa31f6ed7e26"
   },
   "outputs": [],
   "source": [
    "key1, key2 = gpax.utils.get_keys(1)\n",
    "\n",
    "model = gpax.MultiTaskGP(\n",
    "    input_dim=1, data_kernel='Matern',  # standard GP parameters\n",
    "    shared_input_space=False,  # different fidelities have differnet number of observations\n",
    "    num_latents=2,  rank=2,  # number of latent functions Q and rank of matrix W\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    key1,\n",
    "    X,\n",
    "    y,\n",
    "    num_warmup=500 if not SMOKE else NUM_WARMUP // 10,\n",
    "    num_samples=500 if not SMOKE else NUM_SAMPLES // 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a63voHGbc0mP"
   },
   "source": [
    "Make prediction (for the second task/input only) and plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2RhCfytR9ug"
   },
   "outputs": [],
   "source": [
    "# Create a set of inputs for the task/fidelity 2\n",
    "X_test2 = np.column_stack((X_full_range, np.ones_like(X_full_range)))\n",
    "\n",
    "# Make a prediction with the trained model\n",
    "y_mean2, y_sampled2 = model.predict(key2, X_test2, noiseless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "ybQfmVf5jcSj",
    "outputId": "4797eb99-ab53-47d6-a1f8-151b206169ed"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.plot(X_full_range, y_mean2, c='red',label=\"Multi-fidelity GP prediction\")\n",
    "plt.fill_between(X_full_range,\n",
    "                y_mean2 - y_sampled2.squeeze().std(0),\n",
    "                y_mean2 + y_sampled2.squeeze().std(0),\n",
    "                alpha=0.3, color='red')\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='Ground Truth')\n",
    "plt.plot(X1, y1, 'b-', label='Theoretical Model (Fidelity 1)', alpha=0.6)\n",
    "plt.scatter(X2, y2, c='k', label='Experimental Data (Fidelity 2)', alpha=0.6)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3Pl2KlFfxLJ"
   },
   "source": [
    "## Theory-informed Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZqu8gTpPwQF"
   },
   "source": [
    "Here demonstrate a theory-guided BO when theoretical simulations are available beforehand and are used to guide the measurements through a multi-task/fidelity GP. This can be used as an alternative solution to structured GP in situations where a mean function is too costly to compute at each step or it is expressed through some complex program that is not fully differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKOlnQlE6VNm"
   },
   "outputs": [],
   "source": [
    "# we'll need these modules for setting up custom priors\n",
    "import numpyro.distributions as dist\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzEUWp1VHSFB"
   },
   "outputs": [],
   "source": [
    "seed = 1 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlDkNnnVP9Ij"
   },
   "source": [
    "Let's prepare datasets. We are going to use a slightly modified Forrester function. The idea is that we have theoretical simulations that approximately describe the true function and we also have sparse experimental observations. We want to find the true minimum from as few additional measurements as possible while using theory as a guide.\n",
    "\n",
    "*Please note that our data is synthetic and our \"simulations\" and \"experiment\" are not actual simulations and actual measurements. However, you can susbtitute them with your actual data and run the same code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "RPZ-r1EHEIBS",
    "outputId": "82970c92-dfb8-418d-8121-d6f515e20a5e"
   },
   "outputs": [],
   "source": [
    "# Define data geberation functions\n",
    "func = lambda x: (6*x - 2)**2 * np.sin(12*x - 4)\n",
    "\n",
    "def y1_func(x, noise=0):\n",
    "    return func(x) + np.random.normal(0, noise, x.shape)\n",
    "\n",
    "def y2_func(x, noise=0):\n",
    "    return func(x + 0.05) + 5 * (x - 0.2) + np.random.normal(0, noise, x.shape)\n",
    "\n",
    "# Generate points in a 1D parameter space\n",
    "X_full_range = np.linspace(-0.2, 1, 100)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(seed)  # for reproducibility\n",
    "\n",
    "obs_noise = 0.2  # observational noise\n",
    "\n",
    "# Fidelity 1 - \"theoretical model\"\n",
    "y1 = y1_func(X_full_range)\n",
    "\n",
    "# Fidelity 2 - \"experimental measurement.\" Data is available only for some points\n",
    "X2_idx = [5, 30]\n",
    "X2 = X_full_range[X2_idx]\n",
    "y2 = y2_func(X2, noise=obs_noise)\n",
    "\n",
    "# Ground truth for Fidelity 2\n",
    "y2_true = y2_func(X_full_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Plot the true y1 function\n",
    "plt.scatter(X_full_range, y1, c='b', alpha=0.5, s=20, label='Simulation')\n",
    "# Plot the true y2 function\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True function')\n",
    "# Plot the observed y2 data\n",
    "plt.scatter(X2, y2, c='k', s=100, marker='x', lw=3, label='Observations')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPZSL5zX5VXW"
   },
   "source": [
    "Define a utility function for performing both vanilla GP-BO and multi-task GP-BO steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEOO8I4h8JcS"
   },
   "outputs": [],
   "source": [
    "def step(gp_model, X_measured, y_measured, X_unmeasured):\n",
    "    \n",
    "    # Get random number generator keys for training and prediction\n",
    "    rng_key1, rng_key2 = gpax.utils.get_keys()\n",
    "    \n",
    "    # Run HMC to obtain posterior samples for the GP model parameters\n",
    "    gp_model.fit(\n",
    "        rng_key1,\n",
    "        X_measured,\n",
    "        y_measured,\n",
    "        num_warmup=500 if not SMOKE else NUM_WARMUP // 10,\n",
    "        num_samples=500 if not SMOKE else NUM_SAMPLES // 10,\n",
    "    )\n",
    "    \n",
    "    # Get predictions (we don't need this step for optimization - only for visualization purposes)\n",
    "    y_pred, y_sampled = gp_model.predict(rng_key2, X_unmeasured, noiseless=True)\n",
    "    \n",
    "    # Compute acquisition function\n",
    "    obj = gpax.acquisition.UCB(\n",
    "        rng_key2,\n",
    "        gp_model,\n",
    "        X_unmeasured,\n",
    "        beta=4,\n",
    "        maximize=False,\n",
    "        noiseless=True\n",
    "    )\n",
    "\n",
    "    return obj, (y_pred, y_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irNGf3sFEhDH"
   },
   "source": [
    "First, let's run a regular GP that uses only experimental observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1scZWD1YEjsp"
   },
   "outputs": [],
   "source": [
    "# Initial points\n",
    "X_measured = X2.copy()\n",
    "y_measured = y2.copy()\n",
    "\n",
    "# The unmeasured points\n",
    "X_unmeasured = np.delete(X_full_range, X2_idx)\n",
    "\n",
    "num_seed_points = len(X_measured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQGfIHHJbvdC"
   },
   "source": [
    "We are going to define custom prior distributions for the GP kernel lengthscale and the model noise. Please note that this step is not mandatory—you're free to leave them undefined (set to None). However, implementing this can significantly improve convergence. The custom prior distribution over the lengthscale takes into account that our entire X-range is confined between 0 and 1. Moreover, based on our knowledge of the approximate noise level in our observations we can accordingly assign a custom noise prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ4wyFilyrMd"
   },
   "outputs": [],
   "source": [
    "# Prior distribution over the kernel lengthscale\n",
    "lenghtscale_prior = dist.Gamma(2, 5)\n",
    "\n",
    "# Prior distribution over the model noise\n",
    "noise_prior = dist.HalfNormal(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAZwXNbVfYxo"
   },
   "source": [
    "Run the standard GP-BO loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dkfJEuTaEwKE",
    "outputId": "8527d782-bcbe-4b9a-bcb5-2db8daba9f22"
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "\n",
    "np.random.seed(seed) # for reproducibility\n",
    "\n",
    "for e in range(num_steps):\n",
    "    print(\"\\nStep {}/{}\".format(e+1, num_steps))\n",
    "\n",
    "    # Initialize vanilla GP model\n",
    "    gp_model = gpax.ExactGP(\n",
    "        input_dim=1,\n",
    "        kernel='Matern',\n",
    "        noise_prior_dist=noise_prior,\n",
    "        lengthscale_prior_dist=lenghtscale_prior,\n",
    "    )\n",
    "\n",
    "    # Single BO step computes acquisition function\n",
    "    acq, (y_pred, y_sampled) = step(gp_model, X_measured, y_measured, X_unmeasured)\n",
    "    \n",
    "    # Get the next point to evaluate\n",
    "    idx = acq.argmax()\n",
    "    next_point = X_unmeasured[idx:idx+1]\n",
    "    \n",
    "    # Measure the point\n",
    "    next_point_value = y2_func(next_point, noise=obs_noise)\n",
    "    \n",
    "    # Update measured data\n",
    "    X_measured = np.append(X_measured, X_unmeasured[idx:idx+1])\n",
    "    y_measured = np.append(y_measured, next_point_value)\n",
    "\n",
    "    # Plot observed points, mean prediction, and acqusition function\n",
    "    lower_b = y_pred - y_sampled.std(axis=(0,1))\n",
    "    upper_b = y_pred + y_sampled.std(axis=(0,1))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4), sharex=True)\n",
    "    ax1.scatter(X_measured[:-1], y_measured[:-1], marker='x', c='k', label=\"Observations\", s=64)\n",
    "    ax1.plot(X_unmeasured, y_pred, lw=2, c='b', label='Posterior mean')\n",
    "    ax1.fill_between(X_unmeasured, lower_b, upper_b, color='b', alpha=0.3, label=\"Model uncertainty\", linewidth=0)\n",
    "    ax2.plot(X_unmeasured, acq, lw=2, c='orangered', label='Acquisition function')\n",
    "    ax2.scatter(X_unmeasured[idx], acq[idx], s=90, c='orangered', label='Next point to measure')\n",
    "    for ax in fig.axes:\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "    ax1.set_ylabel(\"$y$\", fontsize=16)\n",
    "    ax2.set_xlabel(\"$X$\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Update array of unmeasured points ny removing coordinates of the point we just measured\n",
    "    X_unmeasured = np.delete(X_unmeasured, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2jIK1iafblG"
   },
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "UvglPLGVGCbE",
    "outputId": "26c9180d-2d29-46e4-d33e-757337cceeaf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True function')\n",
    "plt.scatter(X_measured[num_seed_points:], y_measured[num_seed_points:], s=30,\n",
    "            c=np.arange(1, len(X_measured[num_seed_points:])+1), label='Sampled points')\n",
    "plt.colorbar(label='Step')\n",
    "plt.scatter(X_measured[:num_seed_points], y_measured[:num_seed_points], s=30,\n",
    "            marker='x', label='Initial points', c='k')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rutL2lr3IzAc"
   },
   "source": [
    "Now let's run the multi-task GP-BO. At the data preparation step, we need to add indices to each $X$ point that will inform the multi-task GP whether the corresponding target $y$ comes from theoretical simulations (index 0) and experimental observations (index 1). When running BO, we will be appending new measurements to ```X_measured_all``` and ```y_measured_all```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_pvdHvpHC5L",
    "outputId": "b7c098e8-b80b-4ca4-da71-30731ef89af7"
   },
   "outputs": [],
   "source": [
    "# Assign indices to simulation and experimental points and concatenate them together\n",
    "X_measured_all = np.vstack(\n",
    "    (np.column_stack((X_full_range.copy(), np.zeros_like(X_full_range))),  # \"theory\" data points\n",
    "     np.column_stack((X2.copy(), np.ones_like(X2))))  # \"experimental\" data points\n",
    ")\n",
    "\n",
    "# Cocnatenate the corresponding targets\n",
    "y_measured_all = np.concatenate([y1.copy(), y2.copy()]).squeeze()  # [theory, experiment]\n",
    "\n",
    "# Array of unmeasured points\n",
    "X_unmeasured = np.delete(X_full_range, X2_idx)\n",
    "\n",
    "# We are interested in predicting only the values associated with the 2nd task\n",
    "X_unmeasured2 = np.column_stack([X_unmeasured, np.ones_like(X_unmeasured)])\n",
    "\n",
    "num_seed_points = len(X_measured_all[len(y1):])\n",
    "\n",
    "print(X_measured_all.shape, y_measured_all.shape, X_unmeasured2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oorqb60RfrX_"
   },
   "source": [
    "Same as before, we define a custom noise prior. The noise prior for experimental data is the same as in the single output case (vanilla GP), but we also added a prior for a potential small noise in simulated data. We will be re-using the same prior over the kernel lengthscale since we work with the same data points and input space.\n",
    "\n",
    "For multi-output GP, we can also specify a prior over the correlation matrix $W$ in the task kernel. Again, this step is optional, but it can help with convergence. Since we expect that there is at least some correlation between two outputs, we center a prior normal distribution over elements of the $W$ matrix at ones with a standard deviation of two. Feel free to adjust it, including assigning different values to the diagonal and off-diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_WZHqlEeuvK"
   },
   "outputs": [],
   "source": [
    "multitask_noise_prior = dist.HalfNormal(\n",
    "    jnp.array([0.1, 0.5])) # (task 1, task 2)\n",
    "\n",
    "W_prior_dist = dist.Normal(\n",
    "    jnp.ones(shape=(2, 2, 2)),  # loc (num_latents, num_tasks, rank)\n",
    "    2*jnp.ones(shape=(2, 2, 2)) # var (num_latents, num_tasks, rank)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "he6yEABtHtoh",
    "outputId": "dc7421fa-db7d-4713-d708-f4c59b263f28"
   },
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "\n",
    "np.random.seed(seed) # for reproducibility\n",
    "\n",
    "for e in range(num_steps):\n",
    "    print(\"\\nStep {}/{}\".format(e+1, num_steps))\n",
    "    gp_model = gpax.MultiTaskGP(\n",
    "        input_dim=1,\n",
    "        data_kernel='Matern',\n",
    "        shared_input_space=False,\n",
    "        num_latents=2,\n",
    "        rank=2,\n",
    "        noise_prior_dist=multitask_noise_prior,\n",
    "        lengthscale_prior_dist=lenghtscale_prior,\n",
    "        W_prior_dist=W_prior_dist\n",
    "    )\n",
    "\n",
    "    # Compute acquisition function\n",
    "    acq, (y_pred, y_sampled) = step(\n",
    "        gp_model,\n",
    "        X_measured_all,\n",
    "        y_measured_all,\n",
    "        X_unmeasured2\n",
    "    )\n",
    "    \n",
    "    # Get the next point to evaluate\n",
    "    idx = acq.argmax()\n",
    "    next_point = X_unmeasured2[idx:idx+1, 0]\n",
    "    \n",
    "    # Measure the point\n",
    "    next_point_value = y2_func(next_point, noise=obs_noise)\n",
    "    \n",
    "    # Update measured data\n",
    "    X_measured_all = np.append(X_measured_all, X_unmeasured2[idx:idx+1], axis=0)\n",
    "    y_measured_all = np.append(y_measured_all, next_point_value)\n",
    "\n",
    "    # Plot observed points, mean prediction, and acqusition function\n",
    "    lower_b = y_pred - y_sampled.std(axis=(0,1))\n",
    "    upper_b = y_pred + y_sampled.std(axis=(0,1))\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4), sharex=True)\n",
    "    ax1.scatter(X_measured_all[len(y1):-1, 0], y_measured_all[len(y1):-1], marker='x', c='k', label=\"Observations\", s=64)\n",
    "    ax1.plot(X_unmeasured2[:,0], y_pred, lw=2, c='b', label='Posterior mean')\n",
    "    ax1.fill_between(X_unmeasured2[:,0], lower_b, upper_b, color='b', alpha=0.3, label=\"Model uncertainty\", linewidth=0)\n",
    "    ax2.plot(X_unmeasured2[:,0], acq, lw=2, c='orangered', label='Acquisition function')\n",
    "    ax2.scatter(X_unmeasured2[idx][0], acq[idx], s=90, c='orangered', label='Next point to measure')\n",
    "    for ax in fig.axes:\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "    ax1.set_ylabel(\"$y$\", fontsize=16)\n",
    "    ax2.set_xlabel(\"$X$\", fontsize=16)\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "    # Update the array of unmeasured point by removing the point that we have just measured\n",
    "    X_unmeasured2 = np.delete(X_unmeasured2, idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "_Yz3urpyJ4N0",
    "outputId": "184a105b-9ac6-4216-f50b-4a7a2e3aa7e9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True function')\n",
    "plt.scatter(X_measured_all[(len(y1)+num_seed_points):, 0],  y_measured_all[(len(y1)+num_seed_points):],\n",
    "            s=100, c=np.arange(1, len(X_measured_all[(len(y1)+num_seed_points):, 0])+1), label='Sampled points')\n",
    "plt.colorbar(label='Step')\n",
    "plt.scatter(X_measured_all[len(y1):len(y1)+num_seed_points, 0],  y_measured_all[len(y1):len(y1)+num_seed_points],\n",
    "            s=100, marker='x', label='Initial points', c='k', lw=3)\n",
    "plt.scatter(X_full_range, y1, c='b', alpha=0.5, s=20, label='Simulation')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nljZKNvhlGls"
   },
   "source": [
    "The \"theory-informed\" GP-BO has clearly outperformed vanilla BO on this case. Note that the selection of the initial (\"seed\") points was somewhat unfavorable to a purely data-driven optimization method. We did this intentionally to highlight how a multi-output GP successfully avoids local minima due to the information it garners from theoretical models or simulations. We posit that this methodology could prove advantageous for researchers aiming to explore or optimize certain physical properties, especially in higher-dimensional or more complex parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQax-mjSmQn6"
   },
   "source": [
    "Let's also get a prediciton over the full range and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H64R1XpzTXMD",
    "outputId": "ebedf9cc-557e-4d1b-d74d-d286ac5a20fa"
   },
   "outputs": [],
   "source": [
    "gp_model = gpax.MultiTaskGP(\n",
    "    input_dim=1,\n",
    "    data_kernel='Matern',\n",
    "    shared_input_space=False,\n",
    "    num_latents=2,\n",
    "    rank=2,\n",
    "    noise_prior_dist=multitask_noise_prior,\n",
    "    lengthscale_prior_dist=lenghtscale_prior,\n",
    "    W_prior_dist=W_prior_dist\n",
    ")\n",
    "\n",
    "# Compute acquisition function\n",
    "_, (y_pred, y_sampled) = step(\n",
    "    gp_model,\n",
    "    X_measured_all,\n",
    "    y_measured_all,\n",
    "    np.column_stack([X_full_range, np.ones_like(X_full_range)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TgRlfTgMeFbv",
    "outputId": "ff031870-597f-4436-d066-6f954b576206"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(6, 3))\n",
    "cax = ax.scatter(X_measured_all[(len(y1)+num_seed_points):, 0],  y_measured_all[(len(y1)+num_seed_points):],\n",
    "            s=150, c=np.arange(1, len(X_measured_all[(len(y1)+num_seed_points):, 0])+1), label='Predicted points', alpha=0.65)\n",
    "cbar = fig.colorbar(cax)\n",
    "cbar.set_label(\"Step\")\n",
    "ax.scatter(X_measured_all[len(y1):len(y1)+num_seed_points, 0],  y_measured_all[len(y1):len(y1)+num_seed_points],\n",
    "            s=150, marker='x', label='Initial points', c='k')\n",
    "ax.plot(X_full_range, y2_true, 'k--', label='True function')\n",
    "ax.plot(X_full_range, y1, 'b', lw=2, label='Simulation')\n",
    "ax.legend()\n",
    "for yi in y_sampled:\n",
    "    ax.plot(X_full_range, yi.mean(0), lw=.3, zorder=0, c='r', alpha=.1)\n",
    "l, = ax.plot(X_full_range, y_sampled[0].mean(0), lw=1, c='r', alpha=1, label=\"Posterior samples\")\n",
    "ax.legend(loc='upper left')\n",
    "l.set_alpha(0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
