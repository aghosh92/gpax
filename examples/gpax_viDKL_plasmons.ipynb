{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ziatdinovmax/gpax/blob/main/examples/gpax_viDKL_plasmons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For github continuous integration only\n",
    "# Please ignore if you're running this notebook!\n",
    "import os\n",
    "if os.environ.get(\"CI_SMOKE\"):\n",
    "    SMOKE = True\n",
    "else:\n",
    "    SMOKE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRlYq61DplJ0"
   },
   "source": [
    "# Deep kernel learning for data reconstruction and automated experiment\n",
    "\n",
    "*Prepared by Maxim Ziatdinov (January 2022). Last updated in October 2023.*\n",
    "\n",
    "This notebook demonstrates how to use deep kernel learning (DKL) for data reconstruction and automated experiments. We will be using a pre-acquired STEM-EELS experimental dataset on plasmonic nanoparticles from Kevin Roccapriore.\n",
    "\n",
    "The DKL can be viewed as an extension of Gaussian process (GP) for high-dimensional data. Hence, we first briefly overview the standard GP. GP is a powerful tool for reconstructing with quantified uncertainty an unknown  (‚Äúblack-box‚Äù) function  over a low-dimensional parameter space. Formally, given a set of observed data points $(x_i, y_i)_{i=1, ..., N}$ and assuming normally distributed observation noise $ùúÄ$, the GP can be defined as\n",
    "\n",
    "$$y_i= f(x_i) + ùúÄ_i$$\n",
    "$$f ‚àº MVN(0; K(x_i, x_j))$$\n",
    "\n",
    "where $MVN$ stands for Multivariate Normal Distribution, $K$ is a kernel function,  and $f(x)$ is a \"black-box\" function we try to reconstruct. The functional form of the kernel $K$ is chosen prior to the experiment, and its hyperparameters are inferred from the observations using either Markov chain Monte Carlo methods or stochastic variational inference. \n",
    "\n",
    "Once the GP model is trained, we can use it to make a probabilistic prediction on new inputs. Specifically, the predictive mean and covariance on the new data $X_*$ are given by\n",
    "\n",
    "$$ùúá_*=K(X_*,X|ùúÉ)K(X,X|ùúÉ)^{-1}y$$\n",
    "$$ùõ¥_*=K(X_*,X_*|ùúÉ)-K(X_*,X|ùúÉ)K(X,X|ùúÉ)^{-1}K(X,X_*|ùúÉ)$$\n",
    "\n",
    "where ùúÉ are learned GP kernel parameters. Note that we absorbed model noise into the computation of kernel function. The predictive uncertainty is given by the square root of the diagonal elements of the covariance matrix $U_*=\\sqrt{\\text{diag}(ùõ¥_*)}$.\n",
    "\n",
    "The GP predictive mean and uncertainty can then be used to derive an acquisition function for selecting the next point to measure. Depending on the form of acqusition function, this can be referred to as active learning (discovering the overall data distribution) or Bayesian optimization (identifying global maximum or minimum). Here, we will refer to both of them as \"active learning.\"\n",
    "\n",
    "A significant limitation of the standard GP (and GP-based active learning) is that it does not scale well with the dimensionality of the parameter space. Another limitation is that the standard GP\n",
    "does not, strictly speaking, learn representations of data which precludes us from using information from different experimental modalities to assist in selecting the next measurement point.\n",
    "\n",
    "To address these issues, we have adapted a [deep kernel learning](https://arxiv.org/abs/1511.02222) (DKL) approach. DKL can be understood as a hybrid of deep neural network (DNN) and GP. The DNN serves as a feature extractor that allows reducing the complex high-dimensional features to low-dimensional descriptors on which a standard GP kernel operates. The parameters of DNN and GP kernel are inferred jointly in an end-to-end fashion. Practically, the DKL training inputs are usually patches from an (easy-to-acquire) structural image over a large field of view, and training targets represent a physical property of interest derived from the (hard-to-acquire) spectra measured in those patches. The DKL output on the new inputs (image patches for which there are no measured spectra) is the expected property value and associated uncertainty, which can be used to derive the next measurement point in the automated experiments. \n",
    "\n",
    "Formally, we define a deep kernel as\n",
    "\n",
    "$$k_{DKL}(x_i,x_j|w,ùúÉ)=k_{base}(g(x_i|w),g(x_j|w)|ùúÉ)$$\n",
    "\n",
    "where $g$ is a neural network with weights $w$ and $k_{base}$ is a standard GP kernel. The parameters of neural network and GP base\n",
    "kernel are learned simultaneously via Markov chain Monte Carlo sampling techniques or a stochastic variational inference. The trained DKL model is then used for obtaining predictive mean and uncertainty and deriving the acquisition function the same way as for the standard GP.\n",
    "\n",
    "GPax package has the fully Bayesian DKL (weights of neural network and GP hyperparameters are inferred using Hamiltonian Monte Carlo) and the Variational Inference approximation of DKL, viDKL. The fully Bayesian DKL can provide an asymptotically exact solution but is too slow for most automated experiments. Hence, for the latter, we use the viDKL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdtH0tCPQ2de"
   },
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86iUwKxLO7qE"
   },
   "source": [
    "Install the latest GPax package from PyPI (this is best practice, as it installs the latest, deployed and tested version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ1rLUzqha2i",
    "outputId": "44157aab-4e21-4966-ec79-ccf85cd4bbaa"
   },
   "outputs": [],
   "source": [
    "!pip install gpax\n",
    "!pip install atomai  # we will use the atomai's utility function for preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vygoK7MTjJWB"
   },
   "source": [
    "Import needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For use on Google Colab\n",
    "    import gpax\n",
    "\n",
    "except ImportError:\n",
    "    # For use locally (where you're using the local version of gpax)\n",
    "    print(\"Assuming notebook is being run locally, attempting to import local gpax module\")\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    import gpax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rhw-FGD_cAQo"
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from atomai.utils import get_coord_grid, extract_patches_and_spectra\n",
    "\n",
    "gpax.utils.enable_x64()\n",
    "\n",
    "filterwarnings(\"ignore\", module=\"haiku._src.data_structures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable some pretty plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egEExnWqZN4e"
   },
   "source": [
    "## Prepared data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3m72GfO1hji"
   },
   "source": [
    "Download training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaDtfIvHmb42"
   },
   "outputs": [],
   "source": [
    "!wget -qq https://www.dropbox.com/s/1tguc2zraiyxg7h/Plasmonic_EELS_FITO0_edgehole_01.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpmXAAcm1epo"
   },
   "source": [
    "Load data into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Zd9NPigT-3e",
    "outputId": "ae17064c-1bb5-430c-c640-fbdb2444aaa0"
   },
   "outputs": [],
   "source": [
    "loadedfile = np.load(\"Plasmonic_EELS_FITO0_edgehole_01.npy\", allow_pickle=True).tolist()\n",
    "for key, value in loadedfile.items():\n",
    "  print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWB_i6sL0hxd"
   },
   "outputs": [],
   "source": [
    "img = loadedfile['image']\n",
    "specim = loadedfile['spectrum image']\n",
    "e_ax = loadedfile['energy axis']\n",
    "imscale = loadedfile['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eMPatXhjayA"
   },
   "source": [
    "Generate training inputs (image patches) and targets (spectra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IU1nT-SAVLRv",
    "outputId": "a1471c22-b8c2-4a73-c910-2bd023b1916c"
   },
   "outputs": [],
   "source": [
    "window_size = 16\n",
    "\n",
    "coordinates = get_coord_grid(img, step=1, return_dict=False)\n",
    "features, targets, indices = extract_patches_and_spectra(\n",
    "    specim,\n",
    "    img,\n",
    "    coordinates=coordinates,\n",
    "    window_size=window_size,\n",
    "    avg_pool=16\n",
    ")\n",
    "\n",
    "features.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pse4bz08ji_w"
   },
   "source": [
    "Normalize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6j89EEl0nTx"
   },
   "outputs": [],
   "source": [
    "norm_ = lambda x: (x - x.min()) / x.ptp()\n",
    "features, targets = norm_(features), norm_(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anfR3ms8ZYhq"
   },
   "source": [
    "### Scalarizer\n",
    "Next, we select a scalarizer function that will convert a measured spectrum into a scalar physical descriptor. The scalarizer defines the measure of physical interest to the response and acts as a reward towards the specific behavior in the experiment. The scalarizer can be created in almost unlimited number of ways. For example, it can be based\n",
    "on the gross characteristic of the spectrum, such as area under curve, integrated intensity within certain energy range, area or width of hysteresis loop, etc. It can incorporate the physical model and physics-based analysis, for example converting predicted spectrum to specific materials parameters. It can be based on hybrid criteria defined via combinations of functional fits, decision trees, etc. Finally, it can be crowd sourced ‚Äì if the neural network has been trained by human labelled data, or other form of expert system can be used. Ultimately however, the scalarizer should define the measure of physical interest much like the human operator would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dpeNfJIZnG0"
   },
   "source": [
    "Let's scalarize our vactor-valued targets. Here we are going to do the scalarization by simply using the intensity of the most prominent plasmonic peak, but more sophisticated scalar descritpors can be derived as well (ratio of peaks, etc.). First, let's do it for a single spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "nHxUVIguQjod",
    "outputId": "71a2f18a-e001-48a5-c353-27e0f33233b4"
   },
   "outputs": [],
   "source": [
    "k = 1\n",
    "peak_data = find_peaks(targets[k], width=5)\n",
    "peak_pos, peak_int = peak_data[0][0], peak_data[1]['prominences'][0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "ax.plot(targets[k], zorder=0)\n",
    "print(peak_pos, peak_int)\n",
    "ax.scatter(peak_pos, peak_int, marker='x', s=50, c='k', zorder=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XelSLYh8WRe4"
   },
   "source": [
    "Now for all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCu0VYT0S5Zi"
   },
   "outputs": [],
   "source": [
    "peaks_all, features_all, indices_all = [], [], []\n",
    "for i, t in enumerate(targets):\n",
    "    peak = find_peaks(t, width=5)[1][\"prominences\"]\n",
    "    if len(peak) == 0:\n",
    "        continue\n",
    "    peaks_all.append(np.array([peak[0]]) if len(peak) > 1 else peak)\n",
    "    features_all.append(features[i])\n",
    "    indices_all.append(indices[i])\n",
    "peaks_all = np.concatenate(peaks_all)\n",
    "features_all = np.array(features_all)\n",
    "indices_all = np.array(indices_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5sWa98FkeIk"
   },
   "source": [
    "Plot the scalarized target values for each feature coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "aWJnd_j_Tqfi",
    "outputId": "f5bd2085-d8d6-4bfc-ac40-b4770242049f"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.scatter(indices_all[:, 1], indices_all[:, 0], c=peaks_all)\n",
    "ax.set_title('Plasmon peak intensities')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-pM3h6JiP4B"
   },
   "source": [
    "## Reconstruction from partial data\n",
    "Here we demonstrate how to use DKL to learn a correlative structure-property relationship from a relatively small number of image-(scalarized)spectrum pairs and then use the trained model to predict a targeted physical property for the entire image space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvif-dhCaD7U"
   },
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNnP9jFg0t3S",
    "outputId": "6eaeb98c-8209-470b-e461-507b8fae461b"
   },
   "outputs": [],
   "source": [
    "n, d1, d2 = features_all.shape\n",
    "X = features_all.reshape(n, d1*d2)\n",
    "y = peaks_all\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOP2WywLefsc"
   },
   "source": [
    "Split the data in such a way that we use only a relatively small part of data (to the left of the vertical dashed line in the figure below) to train a DKL model and then use the trained model to make a prediction of the \"unmeasured\" plasmon peak values (the part to the right of the vertical dashed line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "9aey5Pi-eens",
    "outputId": "135a6f85-210a-40ce-a317-dc9246bf5515"
   },
   "outputs": [],
   "source": [
    "split_ = 25\n",
    "X_train = X[indices_all[:, 1] < split_]\n",
    "y_train = y[indices_all[:, 1] < split_]\n",
    "indices_train = indices_all[indices_all[:, 1] < split_]\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(indices_all[:, 1], indices_all[:, 0], s=50, c=peaks_all)\n",
    "ax.vlines(split_, indices_all[:, 0].min(), indices_all[:, 0].max(), linestyle='--', color='w')\n",
    "ax.text(split_ // 2, 20, 'Training data', c='w', size=12, rotation=90)\n",
    "ax.text((split_ + indices[:, 1].max()) // 2, 15, 'Unknown (unmeasured)', c='w', size=12, rotation=90)\n",
    "ax.set_title('Plasmon peak intensities');\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2-UfRVk7ARV"
   },
   "source": [
    "Initialize and train a DKL model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHvaB6xH0e-b",
    "outputId": "1f04aec8-0bfc-45d0-9821-ebc5f1ec35b3"
   },
   "outputs": [],
   "source": [
    "data_dim = X_train.shape[-1]\n",
    "\n",
    "key1, key2 = gpax.utils.get_keys()\n",
    "\n",
    "dkl = gpax.viDKL(data_dim, z_dim=2, kernel='RBF')\n",
    "dkl.fit(key1, X_train, y_train, num_steps=100, step_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF__ldsB8fFM"
   },
   "source": [
    "Use the trained model to make a probabilsitic prediction for all the image patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWcmc-bM0fBN"
   },
   "outputs": [],
   "source": [
    "mean, var = dkl.predict(key2, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qe-4kVCk7GO"
   },
   "source": [
    "Visualize predictive mean and uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "OE_kChBz4_ey",
    "outputId": "aa901e9f-31b4-4548-d0fd-3ee55cf8158a"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 5))\n",
    "ax1.scatter(indices_all[:, 1], indices_all[:, 0], s=50, c=mean)\n",
    "ax1.set_title(\"DKL prediction\")\n",
    "ax2.scatter(indices_all[:, 1], indices_all[:, 0], s=50, c=var)\n",
    "ax2.set_title(\"DKL uncertainty\")\n",
    "ax1.vlines(split_, indices_all[:, 0].min(), indices_all[:, 0].max(), linestyle='--', color='w')\n",
    "ax2.vlines(split_, indices_all[:, 0].min(), indices_all[:, 0].max(), linestyle='--', color='w')\n",
    "for _ax in fig.axes:\n",
    "    _ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKlW9R72lWJs"
   },
   "source": [
    "We can also visualize the latent/embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "56dO8Krs-dj3",
    "outputId": "7d493643-5920-47e6-d393-271bd30b2f98"
   },
   "outputs": [],
   "source": [
    "embeded = dkl.embed(X)\n",
    "embeded = embeded / embeded.max()\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2.5))\n",
    "ax1.scatter(indices_all[:, 1], indices_all[:, 0], c=embeded[:, 0], cmap='RdBu')\n",
    "ax2.scatter(indices_all[:, 1], indices_all[:, 0], c=embeded[:, 1], cmap='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me2CwL18iUQ8"
   },
   "source": [
    "## Active learning\n",
    "Now we are going to use DKL to actively learn (local) structures where a particular physical behavior or property (here, plasmon resonance peak) is maximized. Starting with just a few \"measured\" point, we use DKL to obtain predictive mean and variance for our property of interest over the entire parameter space, and then use them to compute the upper confedence bound (UCB) acquisition function for sampling the next measurement point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsigxGPWEI5a"
   },
   "source": [
    "Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsqZ509dUN3L",
    "outputId": "846f000a-5fc7-4d29-a374-5a7a71ccd839"
   },
   "outputs": [],
   "source": [
    "n, d1, d2 = features_all.shape\n",
    "X = features_all.reshape(n, d1*d2)\n",
    "y = peaks_all\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht5nyGNsEKZ_"
   },
   "source": [
    "Get the initial measurements aka training points. Here ```X_measured``` are the already measured points, that is, the image patches for which there are measured spectra, whose scalarized values are stored in ```y_measured```. The ```X_unmeasured``` are unmeasured points, that is, image patches for which there are yet no measured spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrPrmYR65aF8"
   },
   "outputs": [],
   "source": [
    "# use only 0.02% of grid data points as initial training points\n",
    "(\n",
    "    X_measured, \n",
    "    X_unmeasured,\n",
    "    y_measured,\n",
    "    y_unmeasured,\n",
    "    indices_measured,\n",
    "    indices_unmeasured\n",
    ") = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    indices_all,\n",
    "    test_size=0.998,\n",
    "    shuffle=True,\n",
    "    random_state=1\n",
    ")\n",
    "  \n",
    "seed_points = len(X_measured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "7253PSlRX5vY",
    "outputId": "fe4b446b-7337-4aae-a5ae-bda16cade514"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(indices_measured[:, 1], indices_measured[:, 0], s=50, c=y_measured)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gphSs8wZ4LKs"
   },
   "source": [
    "Do sample exploration based on the pre-acquired data (i.e., we are running a \"dummy\" experiment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(indices, obj):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    ax.scatter(indices[:, 1], indices[:, 0], s=32, c=obj, marker='s')\n",
    "    next_point = indices[obj.argmax()]\n",
    "    ax.scatter(next_point[1], next_point[0], marker='x', c='k')\n",
    "    ax.set_title(\"Acquisition function values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g-MX7N6F4LKs",
    "outputId": "0638538c-ee21-4791-b54a-3b9991c519fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dim = X_measured.shape[-1]\n",
    "\n",
    "exploration_steps = 80 if not SMOKE else 5\n",
    "\n",
    "key1, key2 = gpax.utils.get_keys()\n",
    "\n",
    "for e in range(exploration_steps):\n",
    "    print(\"{}/{}\".format(e+1, exploration_steps))\n",
    "    \n",
    "    # update GP posterior\n",
    "    dkl = gpax.viDKL(data_dim, 2)\n",
    "\n",
    "    # you may decrease step size and increase number of steps\n",
    "    # (e.g. to 0.005 and 1000) for more stable performance\n",
    "    dkl.fit(\n",
    "        key1, X_measured, y_measured, num_steps=100, step_size=0.05\n",
    "    )\n",
    "\n",
    "    # Compute UCB acquisition function\n",
    "    obj = gpax.acquisition.UCB(key2, dkl, X_unmeasured, beta=0.25, maximize=True)\n",
    "\n",
    "    # Select next point to \"measure\"\n",
    "    next_point_idx = obj.argmax()\n",
    "\n",
    "    # Do \"measurement\"\n",
    "    measured_point = y_unmeasured[next_point_idx]\n",
    "\n",
    "    # Plot current result\n",
    "    plot_result(indices_unmeasured, obj)\n",
    "\n",
    "    # Update the arrays of measured/unmeasured points\n",
    "    X_measured = np.append(X_measured, X_unmeasured[next_point_idx][None], 0)\n",
    "    X_unmeasured = np.delete(X_unmeasured, next_point_idx, 0)\n",
    "    y_measured = np.append(y_measured, measured_point)\n",
    "    y_unmeasured = np.delete(y_unmeasured, next_point_idx)\n",
    "    indices_measured = np.append(indices_measured, indices_unmeasured[next_point_idx][None], 0)\n",
    "    indices_unmeasured = np.delete(indices_unmeasured, next_point_idx, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "bcuGIhxQ4lRK",
    "outputId": "0b309abb-6d42-4c9b-b96f-4fac02b223d3"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img, origin=\"lower\", cmap='gray')\n",
    "plt.scatter(\n",
    "    indices_measured[seed_points:, 1],\n",
    "    indices_measured[seed_points:, 0],\n",
    "    c=np.arange(len(indices_measured[seed_points:])),\n",
    "    s=50,\n",
    "    cmap=\"Reds\"\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bOB0EwY6F2N"
   },
   "source": [
    "Overlay with a 'ground truth':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qFnRsp43nM4F",
    "outputId": "58905fa6-e083-4a89-d8fb-6712782b1104"
   },
   "outputs": [],
   "source": [
    "plt.scatter(indices_all[:, 1], indices_all[:, 0], c=peaks_all, cmap='jet', alpha=0.5)\n",
    "plt.scatter(\n",
    "    indices_measured[seed_points:, 1],\n",
    "    indices_measured[seed_points:, 0],\n",
    "    c=np.arange(len(indices_measured[seed_points:])),\n",
    "    s=50,\n",
    "    cmap=\"Greens\"\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp5I53wknQLn"
   },
   "source": [
    "Looks like it did a pretty good job identifying the regions where a physical behavior of interest is maximized."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
