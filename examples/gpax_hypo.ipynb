{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ziatdinovmax/gpax/blob/main/examples/gpax_hypo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For github continuous integration only\n",
    "# Please ignore if you're running this notebook!\n",
    "import os\n",
    "if os.environ.get(\"CI_SMOKE\"):\n",
    "    NUM_WARMUP = 100\n",
    "    NUM_SAMPLES = 100\n",
    "else:\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KswNo4REitip"
   },
   "source": [
    "# Hypothesis learning: toy data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M3tFS1hiebL"
   },
   "source": [
    "This notebook demonstrates how to apply the hypothesis learning to toy data. The [hypothesis learning](https://arxiv.org/abs/2112.06649) is based on the idea that in active learning, the correct model of the system’s behavior leads to a faster decrease in the overall Bayesian uncertainty about the system under study. In the hypothesis learning setup, the probabilistic models of the possible system’s behaviors (hypotheses) are wrapped into structured Gaussian processes, and a basic reinforcement learning policy (such as epsilon-greedy or softmax) is used to select a correct model from several competing hypotheses.\n",
    "\n",
    "*Prepared by Maxim Ziatdinov (2023). Last updated in October 2023.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdtH0tCPQ2de"
   },
   "source": [
    "## Install & Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86iUwKxLO7qE"
   },
   "source": [
    "Install the latest GPax package from PyPI (this is best practice, as it installs the latest, deployed and tested version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ1rLUzqha2i",
    "outputId": "44157aab-4e21-4966-ec79-ccf85cd4bbaa"
   },
   "outputs": [],
   "source": [
    "!pip install gpax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vygoK7MTjJWB"
   },
   "source": [
    "Import needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For use on Google Colab\n",
    "    import gpax\n",
    "\n",
    "except ImportError:\n",
    "    # For use locally (where you're using the local version of gpax)\n",
    "    print(\"Assuming notebook is being run locally, attempting to import local gpax module\")\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    import gpax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "from typing import Union, Dict, Type\n",
    "\n",
    "import gpax\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "import numpyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpax.utils.enable_x64()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable some pretty plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ukjdW3IYiHeu"
   },
   "outputs": [],
   "source": [
    "#@title Plotting and data utilities { form-width: \"20%\" }\n",
    "\n",
    "def update_datapoints(point_idx, point_measured, X_measured, y_measured, X_unmeasured):\n",
    "    \"\"\"Updates \"measured\" and \"unmeasured\" arrays of (dummy) data points\"\"\"\n",
    "    X_measured = jnp.append(X_measured, X_unmeasured[point_idx][None], 0)\n",
    "    X_unmeasured = jnp.delete(X_unmeasured, point_idx, 0)\n",
    "    y_measured = jnp.append(y_measured, point_measured)\n",
    "    return X_measured, y_measured, X_unmeasured\n",
    "    \n",
    "\n",
    "def plot_results(X_measured, y_measured, X_unmeasured, y_pred, y_sampled, obj, model_idx, rewards, **kwargs):\n",
    "    X = jnp.concatenate([X_measured, X_unmeasured], axis=0).sort()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.scatter(X_measured, y_measured, marker='x', s=100, c='k', label=\"Measured points\", zorder=1)\n",
    "    ax1.plot(X, y_pred, c='red', label='Model reconstruction', zorder=0)\n",
    "    ax1.fill_between(X, y_pred - y_sampled.std(0), y_pred + y_sampled.std(0),\n",
    "                     color='r', alpha=0.2, label=\"Model uncertainty\", zorder=0)\n",
    "    ax1.set_xlabel(\"$x$\", fontsize=18)\n",
    "    ax1.set_ylabel(\"$y$\", fontsize=18)\n",
    "    ax2.plot(X_unmeasured, obj, c='k')\n",
    "    ax2.vlines(X_unmeasured[obj.argmax()], obj.min(), obj.max(), linestyles='dashed', label= \"Next point\")\n",
    "    ax2.set_xlabel(\"$x$\", fontsize=18)\n",
    "    ax2.set_ylabel(\"Acquisition function\", fontsize=18)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "    step = kwargs.get(\"e\", 0)\n",
    "    plt.suptitle(\"Step: {},  Sampled Model: {}, Rewards: {}\".format(\n",
    "        step+1, model_idx, onp.around(rewards, 3).tolist()), fontsize=24)\n",
    "    # fig.savefig(\"./{}.png\".format(step))\n",
    "    plt.show() \n",
    "    \n",
    "\n",
    "def plot_acq(x, obj, idx):\n",
    "    plt.plot(x.squeeze(), obj, c='k')\n",
    "    plt.vlines(x[idx], obj.min(), obj.max(), linestyles='dashed')\n",
    "    plt.xlabel(\"$x$\", fontsize=18)\n",
    "    plt.ylabel(\"Acquisition function\", fontsize=18)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_final_result(X, y, X_unmeasured, y_pred, y_sampled, seed_points):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(X[seed_points:], y[seed_points:], c=jnp.arange(1, len(X[seed_points:])+1),\n",
    "                cmap='viridis', label=\"Sampled points\", zorder=2)\n",
    "    cbar = plt.colorbar(label=\"Exploration step\")\n",
    "    cbar_ticks = jnp.arange(2, len(X[seed_points:]) + 1, 2)\n",
    "    cbar.set_ticks(cbar_ticks)\n",
    "    plt.scatter(X[:seed_points], y[:seed_points], marker='x', s=64,\n",
    "                c='k', label=\"Seed points\", zorder=1)\n",
    "    plt.plot(X_unmeasured, y_pred, '--', c='red', label='Model reconstruction', zorder=1)\n",
    "    plt.plot(X_unmeasured, truefunc, c='k', label=\"Ground truth\", zorder=0)\n",
    "    plt.fill_between(X_unmeasured, y_pred - y_sampled.std(0), y_pred + y_sampled.std(0),\n",
    "                            color='r', alpha=0.2, label=\"Model uncertainty\", zorder=0)\n",
    "    plt.xlabel(\"$x$\", fontsize=12)\n",
    "    plt.ylabel(\"$y$\", fontsize=12)\n",
    "    plt.legend(fontsize=9, loc='upper left')\n",
    "    #plt.ylim(1.8, 6.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdXQ-tJXhmUC"
   },
   "source": [
    "First, let's generate some data. As a practical example chosen here, we are interested in the active learning of phase\n",
    "diagram that has a transition between different phases. The phase transition manifests in discontinuity of a measurable system’s property, such as heat capacity. However, we usually do not know where a phase transition occurs precisely, nor are we aware of the exact behavior of the property of interest in different phases. We note that using a standard Gaussian process-based active learning is not an optimal choice in such a case as simple GP struggles around the discontinuity point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Onu_jtyMH3q2",
    "outputId": "a4cfcd80-7004-4911-bc80-7739f5694d65"
   },
   "outputs": [],
   "source": [
    "def function_(x: jnp.ndarray, params: Dict[str, float]) -> jnp.ndarray:\n",
    "    return jnp.piecewise(\n",
    "        x,\n",
    "        [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: x**params[\"beta1\"], lambda x: x**params[\"beta2\"]]\n",
    "    )\n",
    "\n",
    "\n",
    "X = jnp.linspace(0.0, 2.5, 100)\n",
    "params_i = {\"t\": 1.6, \"beta1\": 4, \"beta2\": 2.5}\n",
    "\n",
    "truefunc = function_(X, params_i)\n",
    "Y = truefunc + 0.2 * onp.random.normal(size=len(X))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "ax.scatter(X, Y, alpha=0.5, c='k', label=\"Noisy observations\")\n",
    "ax.plot(X, truefunc, lw=2, c='k', label=\"True function\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RlzwfBpgamd"
   },
   "source": [
    "Of course, our algorithm is not going to see all these observations. Nor it is going to see the true function. Instead, we are going to start with just 4 measured points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "zPAIPhvdglt7",
    "outputId": "ce1c1ec3-ecec-4056-d6a3-e593a37d7778"
   },
   "outputs": [],
   "source": [
    "onp.random.seed(1)\n",
    "\n",
    "seed_idx = onp.array([0, 33, 66, 99])\n",
    "X_measured = X[seed_idx]\n",
    "X_unmeasured = onp.delete(X, seed_idx)\n",
    "y_measured = function_(X_measured, params_i) + 0.2 * onp.random.normal(size=len(X_measured))\n",
    "num_seed_points = len(X_measured)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "ax.scatter(X_measured, y_measured, alpha=1.0, c='k', marker='x', label=\"Noisy observations\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ31U3oPhe8u"
   },
   "source": [
    "Next, we define possible models of system's behavior as dereministic functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdrtXqGPKzUe"
   },
   "outputs": [],
   "source": [
    "def piecewise1(x: jnp.ndarray, params: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"Power law behavior before the transition and linear behavior after the transition\"\"\"\n",
    "    return jnp.piecewise(\n",
    "        x,\n",
    "        [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: x**params[\"beta\"], lambda x: params[\"c\"]*x]\n",
    "    )\n",
    "    \n",
    "def piecewise2(x: jnp.ndarray, params: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"Linear behavior before and after the transition\"\"\"\n",
    "    return jnp.piecewise(\n",
    "        x,\n",
    "        [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: params[\"b\"]*x, lambda x: params[\"c\"]*x]\n",
    "    )\n",
    "    \n",
    "def piecewise3(x: jnp.ndarray, params: Dict[str, jnp.ndarray]) -> jnp.ndarray:\n",
    "    \"\"\"Power-law behavior before and after the transition\"\"\"\n",
    "    return jnp.piecewise(\n",
    "        x,\n",
    "        [x < params[\"t\"], x >= params[\"t\"]],\n",
    "        [lambda x: x**params[\"beta1\"], lambda x: x**params[\"beta2\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6az3H14qKQg"
   },
   "source": [
    "We put priors over parameters of each model to make them probabilistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqXxUSGeqGhm"
   },
   "outputs": [],
   "source": [
    "def piecewise1_priors() -> Dict[str, jnp.ndarray]:\n",
    "    # Sample model parameters\n",
    "    t = numpyro.sample(\"t\", numpyro.distributions.Uniform(0.5, 2.0))\n",
    "    beta = numpyro.sample(\"beta\", numpyro.distributions.Normal(3, 1))\n",
    "    c = numpyro.sample(\"c\", numpyro.distributions.Normal(3, 1))\n",
    "    # Return sampled parameters as a dictionary\n",
    "    return {\"t\": t, \"beta\": beta, \"c\": c}\n",
    "\n",
    "def piecewise2_priors() -> Dict[str, jnp.ndarray]:\n",
    "    # Sample model parameters\n",
    "    t = numpyro.sample(\"t\", numpyro.distributions.Uniform(0.5, 2.0))\n",
    "    b = numpyro.sample(\"b\", numpyro.distributions.Normal(3, 1))\n",
    "    c = numpyro.sample(\"c\", numpyro.distributions.Normal(3, 1))\n",
    "    # Return sampled parameters as a dictionary\n",
    "    return {\"t\": t, \"b\": b, \"c\": c}\n",
    "\n",
    "def piecewise3_priors() -> Dict[str, jnp.ndarray]:\n",
    "    # Sample model parameters\n",
    "    t = numpyro.sample(\"t\", numpyro.distributions.Uniform(0.5, 2.0))\n",
    "    beta1 = numpyro.sample(\"beta1\", numpyro.distributions.Normal(3, 1))\n",
    "    beta2 = numpyro.sample(\"beta2\", numpyro.distributions.Normal(3, 1))\n",
    "    # Return sampled parameters as a dictionary\n",
    "    return {\"t\": t, \"beta1\": beta1, \"beta2\": beta2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KuF0Zd5tgHg"
   },
   "source": [
    "Let's also specify custom priors over GP kernel (this step is optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW7STCxetmVc"
   },
   "outputs": [],
   "source": [
    "def gp_kernel_prior() -> Dict[str, jnp.ndarray]:\n",
    "    length = numpyro.sample(\"k_length\", numpyro.distributions.Uniform(0, 1))\n",
    "    scale = numpyro.sample(\"k_scale\", numpyro.distributions.LogNormal(0, 1))\n",
    "    return {\"k_length\": length, \"k_scale\": scale}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2HhVQcO-RtT"
   },
   "source": [
    "Define a simple reward function for hypothesis learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGn9AeH33kN1"
   },
   "outputs": [],
   "source": [
    "def get_reward(obj_history):\n",
    "    \"\"\"A reward of +/-1 is given if the median uncertainty at the current step\n",
    "    is smaller/larger than the median uncertainty at the previous step\"\"\"\n",
    "    r = 1 if obj_history[-1] < obj_history[-2] else -1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipQJp0Lj_22L"
   },
   "source": [
    "The main part (Algorithm 1 in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W7orp6mL5BEU",
    "outputId": "b430f40f-30a9-4c0c-a184-0ae63321ca7f"
   },
   "outputs": [],
   "source": [
    "exploration_steps = 15\n",
    "warmup_steps = 3\n",
    "plot_reconstruction = True  # available only for exploration phase\n",
    "\n",
    "# Create lists containing physical models and probabilistic priors over their parameters\n",
    "models = [piecewise1, piecewise2, piecewise3]\n",
    "model_priors = [piecewise1_priors, piecewise2_priors, piecewise3_priors]\n",
    "\n",
    "onp.random.seed(1)  # rng seed for reproducibility\n",
    "\n",
    "# Initialize the reward, predictive uncertainty and model selection records\n",
    "record = onp.zeros((len(models), 2))\n",
    "model_choices = []\n",
    "obj_history = []\n",
    "\n",
    "# Warm-up phase\n",
    "for w in range(warmup_steps):\n",
    "    print(\"Warmup step {}/{}\".format(w+1, warmup_steps))\n",
    "    obj_median_all, obj_all = [], []\n",
    "    \n",
    "    # Iterate over probabilistic models in the list\n",
    "    for i, model in enumerate(models):\n",
    "\n",
    "        # for each model, run BI and store uncertainty values\n",
    "        obj, _ = gpax.hypo.step(\n",
    "            model,\n",
    "            model_priors[i],\n",
    "            X_measured,\n",
    "            y_measured,\n",
    "            X_unmeasured,\n",
    "            gp_wrap=True,\n",
    "            gp_kernel='Matern',\n",
    "            gp_kernel_prior=gp_kernel_prior,  # wrap model into a Gaussian process\n",
    "            num_warmup=NUM_WARMUP,\n",
    "            num_samples=NUM_SAMPLES,\n",
    "        )\n",
    "        record[i, 0] += 1\n",
    "        obj_all.append(obj)\n",
    "\n",
    "        # (one can use integral uncertainty instead of median)\n",
    "        obj_median_all.append(jnp.nanmedian(obj).item())\n",
    "\n",
    "    # Reward a model that has the smallest integral/median uncertainty\n",
    "    idx = onp.argmin(obj_median_all)\n",
    "    model_choices.append(idx)\n",
    "    record[idx, 1] += 1\n",
    "\n",
    "    # Store the integral/median uncertainty\n",
    "    obj_history.append(obj_median_all[idx])\n",
    "\n",
    "    # Compute the next measurement point using the predictive uncertainty of rewarded model\n",
    "    obj = obj_all[idx]\n",
    "    next_point_idx = obj.argmax()\n",
    "\n",
    "    # Evaluate the function in the suggested point\n",
    "    measured_point = function_(X_unmeasured[next_point_idx], params_i) + 0.2*onp.random.normal()\n",
    "\n",
    "    # Update arrays with measured and unmeasured points\n",
    "    X_measured, y_measured, X_unmeasured = update_datapoints(\n",
    "        next_point_idx, measured_point, X_measured, y_measured, X_unmeasured\n",
    "    )\n",
    "\n",
    "# Average over the number of warmup steps\n",
    "record[:, 1] = record[:, 1] / warmup_steps\n",
    "\n",
    "# Run exploration phase\n",
    "for e in range(exploration_steps - warmup_steps):\n",
    "    print(\"Exploration step {}/{}\".format(e+warmup_steps+1, exploration_steps))\n",
    "\n",
    "    # Choose model according to epsilon-greedy policy\n",
    "    idx = gpax.hypo.sample_next(record[:, 1], method=\"eps-greedy\", eps=0.4)\n",
    "    model_choices.append(idx)\n",
    "    print(\"Using model {}\".format(idx+1))\n",
    "\n",
    "    # Derive acquisition function with the selected model\n",
    "    obj, m_post = gpax.hypo.step(\n",
    "        models[idx],\n",
    "        model_priors[idx],\n",
    "        X_measured,\n",
    "        y_measured,\n",
    "        X_unmeasured,\n",
    "        gp_wrap=True,\n",
    "        gp_kernel='Matern',\n",
    "        gp_kernel_prior=gp_kernel_prior,  # wrap the sampled model into a Gaussian process\n",
    "        num_restarts=2,\n",
    "        print_summary=False,\n",
    "        num_warmup=NUM_WARMUP,\n",
    "        num_samples=NUM_SAMPLES,\n",
    "    )\n",
    "\n",
    "    # Get reward\n",
    "    obj_history.append(jnp.nanmedian(obj).item())\n",
    "    r = get_reward(obj_history)\n",
    "\n",
    "    # Update records\n",
    "    record = gpax.hypo.update_record(record, idx, r)\n",
    "\n",
    "    # Get the next measurement point from the predictive uncertainty of the sampled model\n",
    "    next_point_idx = obj.argmax()\n",
    "\n",
    "    # Evaluate function in the suggested point\n",
    "    measured_point = function_(X_unmeasured[next_point_idx], params_i) + 0.2*onp.random.normal()\n",
    "    if plot_reconstruction:\n",
    "        \n",
    "        # plot current reconstruction and acqusition function\n",
    "        y_pred, y_sampled = m_post.predict(gpax.utils.get_keys()[1], X)\n",
    "        plot_results(\n",
    "            X_measured,\n",
    "            y_measured,\n",
    "            X_unmeasured,\n",
    "            y_pred,\n",
    "            y_sampled.squeeze(),\n",
    "            obj,\n",
    "            idx+1,\n",
    "            record[:, 1],\n",
    "            e=e+warmup_steps\n",
    "        )\n",
    "\n",
    "    # Update arrays with measured and unmeasured points\n",
    "    X_measured, y_measured, X_unmeasured = update_datapoints(\n",
    "        next_point_idx,\n",
    "        measured_point,\n",
    "        X_measured,\n",
    "        y_measured,\n",
    "        X_unmeasured\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQbb8mOa-V62"
   },
   "source": [
    "Plot integral/median uncerainty as a function of exploration steps:\n",
    "\n",
    "(note that for the warm-up steps, we plot only model that produced lowest uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "0rVeoqj5aiEl",
    "outputId": "89620d6f-33df-487d-c01b-bd0f3565ad63"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 2))\n",
    "cmap = mpl.colormaps[\"rainbow\"].resampled(3)\n",
    "ax.plot(onp.arange(1, exploration_steps+1), obj_history, c='k')\n",
    "for model_index in range(3):\n",
    "    where = onp.where(onp.array(model_choices) == model_index)[0]\n",
    "    ax.scatter(\n",
    "        where + 1,\n",
    "        onp.array(obj_history)[where],\n",
    "        color=cmap(model_index),\n",
    "        s=128,\n",
    "        alpha=1,\n",
    "        label=f\"model {model_index+1}\",\n",
    "    )\n",
    "ax.set_xlabel(\"Exploration step\", fontsize=14)\n",
    "ax.set_ylabel(\"Median uncertainty\", fontsize=14)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85E7O2HN-YFy"
   },
   "source": [
    "View average reward associated with each model:\n",
    "\n",
    "(note that it counts the warmup steps where all the models were evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VO0zellal6n",
    "outputId": "655d1dd8-21ed-48e1-976e-ba65498a19e0"
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(record):\n",
    "    print(\"model {}:  counts {}  reward (avg) {}\".format(i+1, (int(r[0])), onp.round(r[1], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvfndjunkUU3"
   },
   "source": [
    "Compute (and plot) each model's prediction over the entire grid using the final set of the discovered point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "esvnmTqzX0H5",
    "outputId": "098855c7-c022-4be1-853d-0e532e45a561"
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    # use the same parameters as in the main loop\n",
    "    _, gp_model = gpax.hypo.step(\n",
    "        model,\n",
    "        model_priors[i],\n",
    "        X_measured,\n",
    "        y_measured,\n",
    "        gp_wrap=True,\n",
    "        gp_kernel='Matern',\n",
    "        gp_kernel_prior=gp_kernel_prior,\n",
    "        num_restarts=2,\n",
    "        print_summary=0,\n",
    "        num_warmup=NUM_WARMUP,\n",
    "        num_samples=NUM_SAMPLES,\n",
    "    )\n",
    "    y_pred, y_sampled = gp_model.predict(gpax.utils.get_keys()[1], X)\n",
    "    print(\"\\n Model {}, Reward (avg) {}\".format(i+1, onp.round(record[i, 1], 3)))\n",
    "    plot_final_result(\n",
    "        X_measured,\n",
    "        y_measured,\n",
    "        X, y_pred,\n",
    "        y_sampled.squeeze(),\n",
    "        seed_points=num_seed_points\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQelw5QCkZsP"
   },
   "source": [
    "Note that because we wrapped our models in GP and because each model had a transition point, even the first two cases showed a satisfactory fit. At the same time, the model that received the highest reward (i.e. was\n",
    "favored by our algorithm) provided the best fit accompanied by the smallest uncertainty. Hence, we were able to learn a distribution of property of interest with a small number of sparse measurements while also identifying a correct model describing the system’s behavior."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGFBr1eRfS8l2qtBpuqZrr",
   "include_colab_link": true,
   "mount_file_id": "1-ixeuqsSo4uHHfnuQ7zq54N3i_k4H3wf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
